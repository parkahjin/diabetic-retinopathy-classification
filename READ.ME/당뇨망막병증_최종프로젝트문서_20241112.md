# 당뇨망막병증 분류 AI 프로젝트
## 전체 프로젝트 문서 (최종 완성본)

---

**프로젝트 정보**
- **유형:** 의료 AI - 이진 분류 (정상 vs 비정상)
- **데이터셋:** Kaggle Diabetic Retinopathy Detection (35,126개 이미지)
- **Phase 1 모델 정확도:** 76.64%
- **최종 정확도 (TTA + 임계값 최적화):** 77.18%
- **모델 아키텍처:** MobileNetV2 (96×96 입력)
- **프로젝트 완료일:** 2024년 11월 12일

---

## 목차

1. [프로젝트 개요](#1-프로젝트-개요)
2. [데이터 전처리](#2-데이터-전처리)
3. [모델 개발 (Phase 1)](#3-모델-개발-phase-1)
4. [성능 개선](#4-성능-개선)
5. [Grad-CAM 시각화](#5-grad-cam-시각화)
6. [프로젝트 문서화 및 백업](#6-프로젝트-문서화-및-백업)
7. [최종 결과 및 성과](#7-최종-결과-및-성과)
8. [핵심 학습 내용](#8-핵심-학습-내용)

---

## 1. 프로젝트 개요

### 1.1 목표

망막 이미지에서 당뇨망막병증을 자동으로 탐지하는 딥러닝 모델 개발:
- **입력:** 망막 사진 (96×96 RGB)
- **출력:** 이진 분류 (정상 vs 비정상)
- **목표 정확도:** 80%
- **달성 정확도:** 77.18%

### 1.2 도전 과제

1. **클래스 불균형:** 정상 73.5% vs 비정상 26.5%
2. **메모리 제약:** Kaggle 30GB RAM 제한
3. **해상도 제약:** 96×96 저해상도로 학습
4. **의료 AI 특성:** 해석 가능성(Explainability) 필수

### 1.3 프로젝트 환경

**플랫폼:**
- Kaggle Notebooks (30GB RAM, Tesla P100 GPU)
- Google Colab (테스트용)

**기술 스택:**
- TensorFlow/Keras 2.x
- Python 3.11
- OpenCV, NumPy, Pandas
- Matplotlib, Seaborn

**버전 관리:**
- Git/GitHub
- Google Drive 백업

---

## 2. 데이터 전처리

### 2.1 데이터셋 구조

**원본 데이터:**
- **총 이미지:** 35,126개
- **원본 클래스:** 5단계 (Grade 0-4)
  - Grade 0: 정상 (25,810개, 73.5%)
  - Grade 1: 경미 (2,443개, 7.0%)
  - Grade 2: 중등도 (5,292개, 15.1%)
  - Grade 3: 중증 (873개, 2.5%)
  - Grade 4: 증식성 (708개, 2.0%)

### 2.2 이진 분류 변환

```python
# Grade 0 → 정상 (0)
# Grade 1-4 → 비정상 (1)
labels_df['binary_label'] = labels_df['level'].apply(
    lambda x: 0 if x == 0 else 1
)
```

**변환 후 분포:**
| 클래스 | 개수 | 비율 |
|-------|------|------|
| 정상 (0) | 25,810 | 73.5% |
| 비정상 (1) | 9,316 | 26.5% |
| **불균형 비율** | **2.77:1** | |

### 2.3 Train/Validation 분할

```python
from sklearn.model_selection import train_test_split

train_df, val_df = train_test_split(
    labels_df,
    test_size=0.2,
    stratify=labels_df['binary_label'],
    random_state=42
)
```

**분할 결과:**
- **Train:** 28,100개 (80%)
- **Validation:** 7,026개 (20%)
- **방법:** Stratified split (클래스 비율 유지)

### 2.4 오버샘플링

**전략:** 비정상 클래스에 대한 랜덤 오버샘플링

```python
# 클래스별 분리
normal_df = train_df[train_df['binary_label'] == 0]
abnormal_df = train_df[train_df['binary_label'] == 1]

# 비정상을 정상 개수만큼 오버샘플링
abnormal_oversampled = abnormal_df.sample(
    n=len(normal_df),
    replace=True,
    random_state=42
)

# 결합
train_balanced = pd.concat([normal_df, abnormal_oversampled])
train_balanced = train_balanced.sample(frac=1, random_state=42)
```

**균형 잡힌 Train 세트:**
| 클래스 | 개수 | 비율 |
|-------|------|------|
| 정상 | 20,647 | 50.0% |
| 비정상 | 20,647 | 50.0% |
| **총합** | **41,294** | **1:1 균형** ✅ |

**Validation 세트:**
- 원본 분포 유지 (오버샘플링 안 함)
- 실제 데이터 분포 반영

### 2.5 이미지 전처리

**전처리 파이프라인:**

```python
IMG_SIZE = 96

def preprocess_image(image_path):
    # 1. 이미지 로드
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # 2. CLAHE (대비 제한 적응 히스토그램 평활화)
    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    l_clahe = clahe.apply(l)
    
    lab_clahe = cv2.merge([l_clahe, a, b])
    img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)
    
    # 3. 96×96 리사이즈
    img_resized = cv2.resize(img_clahe, (IMG_SIZE, IMG_SIZE))
    
    # 4. 정규화 [0-255] → [0-1]
    img_normalized = img_resized.astype(np.float32) / 255.0
    
    return img_normalized
```

**전처리 기법 선택 이유:**

1. **CLAHE (Contrast Limited Adaptive Histogram Equalization)**
   - 망막 이미지의 대비 향상
   - 혈관 및 병변 가시성 개선
   - Parameters: clipLimit=2.0, tileGridSize=(8,8)

2. **96×96 해상도**
   - 메모리 최적화 (30GB RAM 제약)
   - MobileNetV2는 저해상도에서도 효과적
   - 224×224 대비 1/5.4 메모리 사용

3. **정규화**
   - [0-1] 범위로 변환하여 학습 안정화
   - Float32 정밀도 사용

**메모리 사용량:**
- Train 데이터: ~4.5 GB
- Validation 데이터: ~1.0 GB
- 총합: ~5.5 GB
- 저장 형식: NumPy 배열 (.npy)

### 2.6 전처리된 데이터 저장

```python
# NumPy 배열로 저장
np.save('X_train.npy', X_train)
np.save('y_train.npy', y_train)
np.save('X_val.npy', X_val)
np.save('y_val.npy', y_val)
```

**Kaggle Dataset으로 영구 보관:**
- Dataset 이름: `xy-data`
- 파일 구성: X_train.npy, y_train.npy, X_val.npy, y_val.npy
- 목적: 세션 종료 시에도 데이터 유지

---

## 3. 모델 개발 (Phase 1)

### 3.1 모델 아키텍처

**MobileNetV2 기반 전이 학습:**

```python
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers, models

# Base model
base_model = MobileNetV2(
    input_shape=(96, 96, 3),
    include_top=False,
    weights='imagenet'
)

# Fine-tuning: 마지막 50개 레이어만 학습
FINE_TUNE_LAYERS = 50
for layer in base_model.layers[:-FINE_TUNE_LAYERS]:
    layer.trainable = False

# Custom head
model = models.Sequential([
    layers.Input(shape=(96, 96, 3)),
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid', dtype='float32')
])
```

**모델 구조:**
- **Input:** (96, 96, 3)
- **Base:** MobileNetV2 (ImageNet pretrained)
- **Pooling:** GlobalAveragePooling2D
- **FC Layers:** 256 → 128 → 1
- **Regularization:** Dropout (0.5, 0.3, 0.2), BatchNormalization
- **Output:** Sigmoid (이진 분류)
- **Total Parameters:** ~3.5M

**MobileNetV2 선택 이유:**

✅ **장점:**
- 저해상도(96×96)에 최적화된 경량 아키텍처
- Depthwise Separable Convolution으로 효율적
- 모바일/임베디드 환경 배포 가능
- 메모리 효율적

❌ **거부된 대안:**
- ResNet50: 224×224+ 필요, 96×96에서 성능 저하
- DenseNet121: 과도하게 깊음, 메모리 과다 사용
- EfficientNet: 유사한 메모리 문제

### 3.2 학습 설정

**Focal Loss 구현:**

```python
def focal_loss(gamma=1.5, alpha=0.5):
    def focal_loss_fixed(y_true, y_pred):
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
        
        y_true = tf.cast(y_true, tf.float32)
        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        alpha_t = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)
        focal_weight = tf.pow(1.0 - p_t, gamma)
        focal_loss = -alpha_t * focal_weight * tf.math.log(p_t)
        
        return tf.reduce_mean(focal_loss)
    
    return focal_loss_fixed
```

**컴파일:**

```python
model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss=focal_loss(gamma=1.5, alpha=0.5),
    metrics=['accuracy']
)
```

**하이퍼파라미터:**

| 파라미터 | 값 | 선택 이유 |
|---------|---|----------|
| **손실 함수** | Focal Loss (γ=1.5, α=0.5) | 클래스 불균형 처리 |
| **옵티마이저** | Adam | 적응형 학습률 |
| **초기 학습률** | 0.0005 | 안정적 수렴 |
| **배치 크기** | 10 | 메모리 최적화 |
| **최대 에폭** | 35 | 충분한 학습 시간 |
| **EarlyStopping** | Patience=10 | 과적합 방지 |
| **ReduceLROnPlateau** | Factor=0.5, Patience=3 | 학습률 동적 조정 |

**데이터 증강:**

```python
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomContrast(0.1),
])
```

### 3.3 학습 과정

**학습 진행:**

```
Epoch 1/35
- Train: accuracy=0.5142, loss=0.1657
- Val: accuracy=0.6855, loss=0.1128
→ 모델 저장

Epoch 2/35
- Val: accuracy=0.7249, loss=0.1053
→ 모델 저장 (개선)

Epoch 3/35
- Val: accuracy=0.7344, loss=0.1038
→ 모델 저장

...

Epoch 13/35 ⭐
- Train: accuracy=0.6679, loss=0.1081
- Val: accuracy=0.7664, loss=0.0946
→ 최고 성능! 모델 저장

Epoch 14-23
- 개선 없음, early stopping 대기

Epoch 16/35
- Learning rate 감소: 0.0005 → 0.00025

Epoch 23/35
- Early stopping 작동 (10 에폭 동안 개선 없음)
```

**학습 패턴 분석:**

1. **Phase 1 (Epoch 1-6): 급격한 상승**
   - 68.55% → 75.15% (6.6%p 향상)
   - 모델이 기본 패턴 학습

2. **Phase 2 (Epoch 7-13): 미세 조정**
   - 75.15% → 76.64% (1.49%p 향상)
   - 세부 특징 학습 및 최적화

3. **Phase 3 (Epoch 14-23): 정체 및 종료**
   - 76.64% 이상 개선 없음
   - Learning rate 감소해도 효과 미미

### 3.4 Phase 1 최종 결과

**최고 모델:** Epoch 13/35

```
=================================================================
🎯 Phase 1 최고 성능
=================================================================
검증 정확도: 76.64%
검증 손실: 0.0946
학습 정확도: 66.79%
학습 손실: 0.1081
=================================================================
```

**모델 저장:**
- `best_model_improved.keras` - 전체 모델
- `best_model_improved.weights.h5` - 가중치만
- `history_improved.pkl` - 학습 이력

**Kaggle Dataset 백업:**
- Dataset: `phase1-mobilenet-76-64pct`
- 목적: 영구 보관 및 재사용

---

## 4. 성능 개선

### 4.1 TTA (Test Time Augmentation)

**전략:** 추론 시 5가지 증강을 적용하고 예측을 평균

```python
# 1. 원본
pred_original = model.predict(X_val, batch_size=32)

# 2. 좌우 반전
pred_flip_lr = model.predict(np.flip(X_val, axis=2), batch_size=32)

# 3. 상하 반전
pred_flip_ud = model.predict(np.flip(X_val, axis=1), batch_size=32)

# 4. 좌우+상하 반전
pred_flip_both = model.predict(
    np.flip(np.flip(X_val, axis=1), axis=2), 
    batch_size=32
)

# 5. 밝기 조정
pred_bright = model.predict(np.clip(X_val * 1.1, 0, 1), batch_size=32)

# 앙상블 평균
predictions_tta = (
    pred_original + pred_flip_lr + pred_flip_ud + 
    pred_flip_both + pred_bright
) / 5.0
```

**TTA 효과:**

| 방법 | 정확도 | 개선 |
|-----|-------|------|
| Phase 1 (기본) | 76.64% | - |
| + TTA | **77.07%** | **+0.43%p** |

**TTA가 효과적인 이유:**
- 다양한 관점에서 이미지 평가
- 모델의 불확실성 감소
- 변형에 대한 강건성 향상
- 단순하지만 효과적인 앙상블 기법

### 4.2 임계값 최적화

**전략:** 최적의 분류 임계값 탐색 (기본값 0.5 대신)

```python
best_threshold = 0.5
best_accuracy = 0

# 0.30 ~ 0.70 범위 탐색
for threshold in np.arange(0.30, 0.71, 0.01):
    y_pred = (predictions_tta > threshold).astype(int).flatten()
    accuracy = accuracy_score(y_val, y_pred)
    
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_threshold = threshold

print(f"최적 임계값: {best_threshold}")
print(f"최고 정확도: {best_accuracy:.4f}")
```

**결과:**

| 방법 | 임계값 | 정확도 | 개선 |
|-----|-------|-------|------|
| TTA | 0.500 | 77.07% | - |
| + 임계값 최적화 | **0.490** | **77.18%** | **+0.11%p** |

**최적 임계값 0.490 분석:**
- 기본값 0.5보다 약간 낮음
- 검증 세트가 불균형(73.5% 정상)을 반영
- True Positives 증가 (비정상 케이스 더 잘 탐지)
- False Positives 약간 증가하나 전체 정확도는 향상

### 4.3 최종 성능 비교

| 단계 | 정확도 | 누적 개선 |
|-----|-------|----------|
| **Phase 1 (기본 모델)** | 76.64% | - |
| **+ TTA** | 77.07% | +0.43%p |
| **+ 임계값 최적화** | **77.18%** | **+0.54%p** |

---

## 5. Grad-CAM 시각화

### 5.1 Grad-CAM이란?

**Gradient-weighted Class Activation Mapping:**
- 모델이 이미지의 어느 부분을 보고 판단하는지 시각화
- 의료 AI에서 필수적인 해석 가능성(Explainability) 제공
- CNN의 마지막 Convolutional layer의 gradient 활용

**중요성:**
- 모델의 판단 근거 확인 가능
- 잘못된 학습 패턴 발견
- 의료진의 AI 신뢰도 향상
- 디버깅 및 모델 개선에 활용

### 5.2 구현

**Grad-CAM 함수:**

```python
def simple_gradcam(model, img_array, layer_name='mobilenetv2_1.00_96'):
    """
    Grad-CAM 히트맵 생성
    """
    # Base model 추출
    base_model = model.get_layer(layer_name)
    last_conv_layer = base_model.get_layer('Conv_1')
    
    # Feature extractor
    grad_model = keras.Model(
        inputs=base_model.input,
        outputs=[last_conv_layer.output, base_model.output]
    )
    
    with tf.GradientTape() as tape:
        conv_outputs, base_predictions = grad_model(img_array)
        
        # 전체 모델 통과
        x = base_predictions
        for layer in model.layers[1:]:
            x = layer(x)
        predictions = x
        
        loss = predictions[0, 0]
    
    # Gradients
    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    
    # Weighted combination
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    
    # Normalize
    heatmap = tf.maximum(heatmap, 0)
    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-10)
    
    return heatmap.numpy()
```

**히트맵 오버레이:**

```python
def overlay_heatmap(img, heatmap, alpha=0.4):
    """
    원본 이미지에 히트맵 오버레이
    """
    heatmap = cv2.resize(heatmap, (96, 96))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
    
    img_uint8 = np.uint8(255 * img)
    result = (heatmap * alpha + img_uint8).astype('uint8')
    
    return result
```

### 5.3 시각화 결과

**생성된 이미지:**

1. **gradcam_final.png**
   - 정상 케이스 3개 + 비정상 케이스 3개
   - 각각 원본 / 히트맵 / 오버레이 표시
   - 전체 비교용

2. **gradcam_normal.png**
   - 정상 케이스 3개 상세
   - 3×3 그리드 (원본, 히트맵, 오버레이)

3. **gradcam_abnormal.png**
   - 비정상 케이스 3개 상세
   - 3×3 그리드 (원본, 히트맵, 오버레이)

**관찰 결과:**

**정상 케이스:**
- 히트맵이 전체적으로 고르게 분산
- 특정 병변 부위에 집중하지 않음
- 모델이 "전체적으로 정상" 판단

**비정상 케이스:**
- 히트맵이 특정 영역에 집중
- 출혈, 삼출물, 미세동맥류 위치 강조
- 실제 병변 위치와 대체로 일치
- 모델이 "여기 문제가 있다" 표시

### 5.4 의료 AI 관점의 의의

**해석 가능성(Explainability):**
- 의료진이 AI의 판단 근거를 확인 가능
- "왜 이 진단을 내렸는가?"에 대한 답변 제공
- Black box → White box AI

**임상적 가치:**
- 의사의 진단 보조 도구로 활용
- AI의 신뢰성 검증 가능
- 오진 가능성 파악
- 의료진 교육 자료로 활용

**모델 개선:**
- 잘못된 학습 패턴 발견 (예: 배경에 집중)
- 데이터셋 품질 검증
- 추가 학습 방향 결정

---

## 6. 프로젝트 문서화 및 백업

### 6.1 프로젝트 문서 작성

**생성된 문서:**

1. **당뇨망막병증_프로젝트완전정리_20241111.md**
   - 전체 프로젝트 과정 상세 설명
   - 전처리, 모델 개발, 성능 개선, 결과 분석
   - 약 20KB, Markdown 형식

2. **당뇨망막병증_프로젝트완전정리_20241111.pdf**
   - Markdown의 PDF 버전
   - 전문적인 레이아웃, 한글 폰트 지원
   - 약 159KB, A4 용지 크기
   - 포트폴리오 및 입사 지원용

3. **GitHub_백업_가이드_20241111.md**
   - Git/GitHub 백업 과정 설명
   - 캐글 노트북 → 로컬 → GitHub 전 과정
   - 약 15KB

4. **GitHub_백업_가이드_20241111.pdf**
   - 백업 가이드의 PDF 버전
   - 약 106KB

### 6.2 로컬 백업

**파일 구조:**

```
딥러닝 파일/
├── MobileNetV2 model - 76%/
│   ├── best_model_improved_76.weights.h5
│   ├── final_mobilenet_improved_76.keras
│   ├── history_improved_76.pkl
│   └── mobilenetv2-model-76.ipynb
├── 전처리/
│   ├── X_train.npy (4.4GB)
│   ├── X_val.npy (759MB)
│   ├── y_train.npy (162KB)
│   ├── y_val.npy (28KB)
│   └── 전처리 코드.txt
├── Grad-CAM 결과 이미지/
│   ├── gradcam_final.png
│   ├── gradcam_normal.png
│   └── gradcam_abnormal.png
├── mobilenetv2-model-77.18%.ipynb (최종 코드)
├── 당뇨망막병증_프로젝트완전정리_20241111.md
├── 당뇨망막병증_프로젝트완전정리_20241111.pdf
├── GitHub_백업_가이드_20241111.md
└── GitHub_백업_가이드_20241111.pdf
```

**로컬 저장 위치:**
```
C:\Users\Myj-B0G-12M\Desktop\딥러닝 파일\
```

### 6.3 Git/GitHub 백업

**Git 초기화:**

```bash
cd "C:\Users\Myj-B0G-12M\Desktop\딥러닝 파일"
git init
git config --global user.name "parkahjin"
git config --global user.email "park266692@gmail.com"
```

**.gitignore 설정:**

```
*.npy
*.keras
*.h5
*.pkl
```

**이유:** 대용량 파일(100MB+) GitHub 업로드 불가

**첫 커밋:**

```bash
git add .
git commit -m "Initial commit: 당뇨망막병증 분류 프로젝트 (Phase 1: 76.64%, 최종: 77.18%)"
```

**GitHub Repository 생성:**
- 이름: `diabetic-retinopathy-classification`
- 설명: `당뇨망막병증 분류 AI (MobileNetV2, 77.18% accuracy)`
- 공개: Public (포트폴리오용)
- URL: https://github.com/parkahjin/diabetic-retinopathy-classification

**원격 저장소 연결 및 푸시:**

```bash
git remote add origin https://github.com/parkahjin/diabetic-retinopathy-classification.git
git branch -M main
git push -u origin main
```

**추가 커밋들:**

```bash
# GitHub 백업 가이드 추가
git add GitHub_백업_가이드_20241111.md GitHub_백업_가이드_20241111.pdf
git commit -m "Add: GitHub 백업 가이드 문서"
git push

# 전처리 코드 추가
git add "전처리/전처리 코드.txt"
git commit -m "Add: 전처리 코드"
git push

# Grad-CAM 결과 추가
git add "Grad-CAM 결과 이미지"
git commit -m "Add: Grad-CAM 시각화 결과"
git push
```

**GitHub에 저장된 파일:**
- ✅ 노트북 파일들 (.ipynb)
- ✅ 프로젝트 문서 (MD, PDF)
- ✅ Grad-CAM 이미지
- ✅ 전처리 코드
- ❌ 대용량 데이터 파일 (로컬에만 존재)

### 6.4 Google Drive 백업

**전체 폴더 업로드:**
- 위치: Google Drive / 딥러닝 파일
- 파일 개수: 87개
- 포함: 모든 파일 (데이터, 모델, 문서, 코드)
- 용량: 약 5.2GB
- 상태: ✅ 업로드 완료

**3중 백업 체계:**

| 백업 위치 | 상태 | 내용 | 목적 |
|----------|------|------|------|
| **로컬 PC** | ✅ | 전체 파일 | 즉시 접근 |
| **GitHub** | ✅ | 코드 + 문서 | 버전 관리 + 포트폴리오 |
| **Google Drive** | ✅ | 전체 파일 | 클라우드 백업 |

### 6.5 README.md 작성

**GitHub Repository 첫 화면:**

```markdown
# 당뇨망막병증 분류 AI 프로젝트

## 프로젝트 개요
망막 이미지에서 당뇨망막병증을 탐지하는 이진 분류 AI 모델

## 주요 성과
- **Phase 1 정확도:** 76.64%
- **최종 정확도:** 77.18% (TTA + 임계값 최적화)
- **모델:** MobileNetV2 (96×96 입력)
- **데이터:** 35,126개 망막 이미지

## 기술 스택
- TensorFlow/Keras
- MobileNetV2 (전이 학습)
- CLAHE 전처리
- Focal Loss (클래스 불균형 처리)
- Grad-CAM (모델 해석)

## 주요 기능
1. 망막 이미지 전처리 (CLAHE)
2. 이진 분류 모델 (정상 vs 비정상)
3. TTA 및 임계값 최적화
4. Grad-CAM 시각화

## 파일 구조
- `mobilenetv2-model-77.18%.ipynb` - 최종 코드
- `당뇨망막병증_프로젝트완전정리.pdf` - 프로젝트 문서
- `Grad-CAM 결과 이미지/` - 시각화 결과

## 실행 방법
1. Kaggle에서 Diabetic Retinopathy Detection 데이터셋 다운로드
2. 노트북 실행
3. 결과 확인

## 성능 지표
- Accuracy: 77.18%
- Precision (정상): 78.01%
- Recall (정상): 96.03%
- F1-Score (정상): 86.08%

## 개발 환경
- Kaggle Notebooks (30GB RAM, Tesla P100 GPU)
- TensorFlow 2.x
- Python 3.11

## 작성자
박아진 (parkahjin)
```

---

## 7. 최종 결과 및 성과

### 7.1 성능 요약

**최종 정확도: 77.18%**

| 단계 | 정확도 | 개선 |
|-----|-------|------|
| Phase 1 (기본 모델) | 76.64% | - |
| + TTA | 77.07% | +0.43%p |
| + 임계값 최적화 | **77.18%** | +0.11%p |
| **총 개선** | | **+0.54%p** |

### 7.2 혼동 행렬

```
                예측
                정상    비정상
실제 정상       4,958    205
     비정상     1,398    465
```

| 지표 | 값 |
|-----|---|
| True Negatives (TN) | 4,958 |
| False Positives (FP) | 205 |
| False Negatives (FN) | 1,398 |
| True Positives (TP) | 465 |

### 7.3 클래스별 성능

| 클래스 | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| **정상** | 0.7801 | 0.9603 | 0.8608 | 5,163 |
| **비정상** | 0.6940 | 0.2496 | 0.3672 | 1,863 |
| **정확도** | | | **0.7718** | 7,026 |
| **Macro Avg** | 0.7370 | 0.6049 | 0.6140 | 7,026 |
| **Weighted Avg** | 0.7572 | 0.7718 | 0.7299 | 7,026 |

### 7.4 핵심 인사이트

**강점:**
- ✅ 정상 클래스의 높은 Recall (96.0%): 건강한 환자 식별 탁월
- ✅ 정상 클래스의 좋은 Precision (78.0%): False alarm 낮음
- ✅ 전체 정확도 77.18%: 목표(80%) 근접
- ✅ Grad-CAM으로 해석 가능성 확보

**약점:**
- ⚠️ 비정상 클래스의 낮은 Recall (25.0%): 병변 케이스 놓침
- ⚠️ 클래스 불균형의 영향: 정상 73.5% vs 비정상 26.5%
- ⚠️ False Negative (1,398개): 비정상을 정상으로 오판

**의료 AI 관점:**
- False Negative가 치명적 (병을 놓치는 경우)
- Recall 향상 필요 → 추가 데이터, 앙상블 모델 고려
- Grad-CAM으로 오진 원인 분석 가능

### 7.5 목표 달성도

| 항목 | 목표 | 달성 | 달성률 |
|-----|------|------|--------|
| **정확도** | 80% | 77.18% | 96.5% |
| **완전한 파이프라인** | ✓ | ✓ | 100% |
| **성능 개선 기법** | ✓ | ✓ (TTA, 임계값) | 100% |
| **해석 가능성** | ✓ | ✓ (Grad-CAM) | 100% |
| **문서화** | ✓ | ✓ (PDF, MD) | 100% |
| **버전 관리** | ✓ | ✓ (Git/GitHub) | 100% |

---

## 8. 핵심 학습 내용

### 8.1 기술적 성과

**달성한 것:**
1. ✅ MobileNetV2를 96×96 저해상도에 최적화
2. ✅ Focal Loss로 클래스 불균형 효과적 처리
3. ✅ TTA 구현으로 +0.43%p 향상
4. ✅ 임계값 최적화로 +0.11%p 추가 향상
5. ✅ Grad-CAM으로 해석 가능성 확보
6. ✅ 메모리 제약(30GB) 극복
7. ✅ 완전한 MLOps 파이프라인 구축

### 8.2 중요한 발견

**1. 해상도와 모델 선택의 관계**
- MobileNetV2: 96×96에서 우수한 성능 ✅
- ResNet50/DenseNet: 96×96에서 26%/73% 패턴 발생 ❌
- 교훈: 모델 선택 시 입력 해상도 고려 필수

**2. 클래스 불균형 처리**
- 오버샘플링: Train 세트만 적용 (1:1 균형)
- Focal Loss: 추가 보정 효과
- Validation: 원본 분포 유지 (실제 상황 반영)

**3. 메모리 최적화 전략**
- 배치 크기: 10으로 제한
- 해상도: 96×96 (224×224 대비 1/5 메모리)
- NumPy 저장: .npy 형식으로 빠른 로드
- .gitignore: 대용량 파일 GitHub 제외

**4. 학습 조기 종료의 중요성**
- Early Stopping: Patience=10으로 과적합 방지
- 최적 모델 저장: Epoch 13 시점
- Learning Rate 감소: 정체 시 자동 조정

**5. 성능 개선 기법의 효과**
- TTA: 단순하지만 효과적 (+0.43%p)
- 임계값 최적화: 작지만 의미 있는 개선 (+0.11%p)
- 누적 효과: 작은 개선들이 쌓여 큰 차이

**6. Grad-CAM의 가치**
- 의료 AI에서 필수적
- 모델 신뢰도 향상
- 디버깅 도구로 활용
- 포트폴리오 차별화

### 8.3 실패와 교훈

**실패한 시도들:**
1. ❌ ResNet50, DenseNet121 → 96×96에서 작동 안 함
2. ❌ Phase 2 모델 시도 → 설정 변경 시 성능 급락
3. ❌ 초기 앙상블 시도 → 입력 해상도 불일치
4. ❌ Epoch 13 이후 추가 학습 → 개선 없음
5. ❌ 캐글 Save Version 오류 → 로컬 백업 필요성 인식

**배운 점:**
- 💡 검증된 설정 유지의 중요성
- 💡 체계적 디버깅: 문제 발생 시 단계별 확인
- 💡 재현성: 코드와 설정의 일관성 유지
- 💡 적시 종료: 과적합보다 조기 종료가 낫다
- 💡 다중 백업: 로컬 + GitHub + Google Drive

### 8.4 프로젝트 관리

**효과적이었던 것:**
- ✅ Kaggle Dataset으로 데이터 영구 보관
- ✅ 단계별 문서화 (Markdown)
- ✅ Git 버전 관리
- ✅ 3중 백업 체계
- ✅ 체계적인 파일 구조

**개선이 필요한 것:**
- ⚠️ 초기 계획: 5-class → 2-class 변경 (유연성)
- ⚠️ 시간 관리: 디버깅에 예상보다 많은 시간 소요
- ⚠️ 실험 추적: 더 체계적인 실험 로그 필요

### 8.5 포트폴리오 가치

**이 프로젝트가 보여주는 것:**

1. **의료 AI 이해도**
   - 망막 이미지 처리 경험
   - 클래스 불균형 처리 능력
   - 의료 AI 특성 이해 (해석 가능성)

2. **문제 해결 능력**
   - 메모리 제약 극복
   - 모델 선택 및 최적화
   - 체계적 디버깅

3. **MLOps 경험**
   - 완전한 파이프라인 구축
   - 버전 관리 (Git/GitHub)
   - 문서화 및 백업

4. **기술 스택 활용**
   - TensorFlow/Keras 숙련도
   - OpenCV 이미지 처리
   - 데이터 시각화
   - Git/GitHub 활용

5. **완성도**
   - 전처리부터 배포까지 전 과정
   - 전문적인 문서화
   - 포트폴리오 준비 완료

---

## 9. 향후 개선 방향

### 9.1 단기 개선 (즉시 가능)

**1. 추가 시각화**
- ROC Curve, PR Curve 추가
- Learning curve 상세 분석
- 클래스별 Grad-CAM 비교

**2. Gradio 데모**
- 웹 인터페이스 구현
- 실시간 예측 및 Grad-CAM 표시
- 면접 시 시연 가능

**3. README 보강**
- 데모 GIF 추가
- 설치 및 실행 가이드 상세화
- 라이선스 정보 추가

### 9.2 중장기 개선 (리소스 필요)

**1. 모델 개선**
- 더 큰 해상도 (224×224) 시도
- 앙상블 모델 (MobileNetV2 + EfficientNet)
- 5-class 분류로 확장

**2. 데이터 개선**
- 외부 데이터셋 추가
- 데이터 증강 기법 확대
- 클래스 균형 맞추기

**3. Recall 향상 전략**
- Class weight 조정
- Focal Loss 파라미터 튜닝
- Threshold별 성능 분석

**4. 배포**
- Docker 컨테이너화
- REST API 구현
- 웹 애플리케이션 개발

### 9.3 연구 확장

**1. 다중 클래스 분류**
- Grade 0-4 세부 분류
- 병변 유형별 분류
- 중증도 예측

**2. Object Detection**
- 병변 위치 정확히 표시
- Bounding box 추가
- YOLO/Faster R-CNN 적용

**3. Segmentation**
- 병변 영역 세분화
- U-Net 구조 활용
- 픽셀 단위 분류

---

## 10. 결론

### 10.1 프로젝트 요약

이 프로젝트는 **제약된 환경에서도 체계적인 접근**으로 의미 있는 결과를 도출할 수 있음을 보여줍니다.

**핵심 성과:**
- ✅ 77.18% 정확도 달성 (목표 80%의 96.5%)
- ✅ 완전한 MLOps 파이프라인 구축
- ✅ Grad-CAM으로 해석 가능성 확보
- ✅ 전문적인 문서화 및 포트폴리오 준비

**프로세스의 가치:**
- 정확도 숫자보다 **문제 해결 과정**이 더 중요
- 제약 조건 속에서의 **최적화 경험**
- 의료 AI 특성 이해: **해석 가능성**
- 체계적인 **프로젝트 관리**

### 10.2 의료 AI 관점

**실용성:**
- 77.18%는 의료 AI에서 의미 있는 수준
- Grad-CAM으로 의료진 신뢰 확보
- False Negative 개선 여지 있음

**확장 가능성:**
- 추가 데이터로 성능 개선 가능
- 다른 안과 질환으로 확장 가능
- 실제 임상 환경 적용 고려

### 10.3 포트폴리오로서의 가치

**차별화 요소:**
1. 완전한 end-to-end 프로젝트
2. 의료 AI 특성 이해 (해석 가능성)
3. 제약 조건 극복 경험
4. 전문적인 문서화
5. MLOps 실무 경험

**채용 담당자 관점:**
- "프로젝트를 완수할 수 있는 능력"
- "문제 해결 접근 방식"
- "문서화 및 커뮤니케이션 능력"
- "기술 스택 활용 능력"

### 10.4 개인적 성장

**기술적 성장:**
- TensorFlow/Keras 숙련도 향상
- 의료 이미지 처리 경험
- MLOps 파이프라인 구축 경험
- Git/GitHub 실무 활용

**소프트 스킬:**
- 프로젝트 관리 능력
- 문제 해결 능력
- 문서화 능력
- 끈기와 완성도

### 10.5 마무리

이 프로젝트를 통해 **"AI 모델을 만드는 것"**을 넘어 **"실용적인 AI 시스템을 구축하고 관리하는 것"**의 중요성을 배웠습니다.

**77.18%**라는 숫자보다 중요한 것:
- 전체 파이프라인을 구축한 경험
- 제약 조건 속에서 최선을 다한 과정
- 체계적인 문제 해결 접근
- 완성도 높은 결과물

**AI Platform(의료 AI 회사) 지원을 위한 준비:**
- ✅ 의료 AI 프로젝트 완성
- ✅ 해석 가능성 확보 (Grad-CAM)
- ✅ 전문적인 포트폴리오
- ✅ GitHub 공개 Repository
- ✅ 면접 대비 완료

---

**프로젝트 완료일:** 2024년 11월 12일  
**총 소요 시간:** 약 20시간
- 전처리: 3시간
- 모델 개발: 2시간
- 성능 개선: 1시간
- Grad-CAM: 2시간
- 디버깅: 9시간
- 문서화: 3시간

**최종 산출물:**
- 학습된 모델 (.keras, .h5)
- 전처리된 데이터 (.npy)
- Jupyter 노트북 (.ipynb)
- 프로젝트 문서 (PDF, MD)
- Grad-CAM 시각화 (PNG)
- GitHub Repository
- Google Drive 백업

---

## 부록

### A. 주요 코드 스니펫

**전처리:**
```python
def preprocess_image(image_path):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    l_clahe = clahe.apply(l)
    lab_clahe = cv2.merge([l_clahe, a, b])
    img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)
    
    img_resized = cv2.resize(img_clahe, (96, 96))
    img_normalized = img_resized.astype(np.float32) / 255.0
    
    return img_normalized
```

**Focal Loss:**
```python
def focal_loss(gamma=1.5, alpha=0.5):
    def focal_loss_fixed(y_true, y_pred):
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
        
        y_true = tf.cast(y_true, tf.float32)
        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        alpha_t = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)
        focal_weight = tf.pow(1.0 - p_t, gamma)
        focal_loss = -alpha_t * focal_weight * tf.math.log(p_t)
        
        return tf.reduce_mean(focal_loss)
    
    return focal_loss_fixed
```

**TTA:**
```python
predictions_tta = (
    model.predict(X_val) +
    model.predict(np.flip(X_val, axis=2)) +
    model.predict(np.flip(X_val, axis=1)) +
    model.predict(np.flip(np.flip(X_val, axis=1), axis=2)) +
    model.predict(np.clip(X_val * 1.1, 0, 1))
) / 5.0
```

### B. 참고 자료

**데이터셋:**
- [Kaggle Diabetic Retinopathy Detection](https://www.kaggle.com/c/diabetic-retinopathy-detection)

**논문:**
- Focal Loss for Dense Object Detection (Lin et al., 2017)
- MobileNetV2: Inverted Residuals and Linear Bottlenecks (Sandler et al., 2018)
- Grad-CAM: Visual Explanations from Deep Networks (Selvaraju et al., 2017)

**기술 문서:**
- TensorFlow/Keras Documentation
- OpenCV Documentation
- Git/GitHub Guides

**프로젝트 Repository:**
- https://github.com/parkahjin/diabetic-retinopathy-classification

---

**문서 끝**
